"use strict";(globalThis.webpackChunkdocs_site=globalThis.webpackChunkdocs_site||[]).push([[4228],{6(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"positioning/what-is-gantral","title":"What is Gantral?","description":"Gantral is an open-source AI Execution Control Plane.","source":"@site/../docs/positioning/what-is-gantral.md","sourceDirName":"positioning","slug":"/positioning/what-is-gantral","permalink":"/positioning/what-is-gantral","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"What is Gantral?"},"sidebar":"docsSidebar","previous":{"title":"Expansion Narrative","permalink":"/positioning/expansion-narrative"},"next":{"title":"What Gantral Is Not","permalink":"/positioning/what-gantral-is-not"}}');var r=t(4848),s=t(8453);const o={title:"What is Gantral?"},a="What Is Gantral?",l={},c=[{value:"The Problem Gantral Solves",id:"the-problem-gantral-solves",level:2},{value:"The Execution Authority Gap (Before and After)",id:"the-execution-authority-gap-before-and-after",level:2},{value:"The Core Distinction: Authority vs Intelligence",id:"the-core-distinction-authority-vs-intelligence",level:2},{value:"How Execution Is Governed",id:"how-execution-is-governed",level:2},{value:"Federated Execution (Runner Model)",id:"federated-execution-runner-model",level:2},{value:"The Governance Failure Gantral Addresses",id:"the-governance-failure-gantral-addresses",level:2},{value:"1. Fragmented Execution Control",id:"1-fragmented-execution-control",level:3},{value:"2. Broken Chain of Authority",id:"2-broken-chain-of-authority",level:3},{value:"What Gantral Owns",id:"what-gantral-owns",level:2},{value:"What Gantral Does Not Do",id:"what-gantral-does-not-do",level:2},{value:"How to Think About Gantral",id:"how-to-think-about-gantral",level:2},{value:"When Gantral Is Appropriate",id:"when-gantral-is-appropriate",level:2},{value:"Where to Go Next",id:"where-to-go-next",level:2}];function h(n){const e={a:"a",br:"br",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"what-is-gantral",children:"What Is Gantral?"})}),"\n",(0,r.jsxs)(e.p,{children:["Gantral is an ",(0,r.jsx)(e.strong,{children:"open-source AI Execution Control Plane"}),"."]}),"\n",(0,r.jsxs)(e.p,{children:["It enforces and records ",(0,r.jsx)(e.strong,{children:"execution-time authority"})," in AI-assisted and agentic workflows."]}),"\n",(0,r.jsxs)(e.p,{children:["Gantral exists to ensure that when an AI-enabled system performs a consequential action,\nan organization can later determine \u2014 ",(0,r.jsx)(e.strong,{children:"without relying on trust or narrative"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"what executed"}),"\n",(0,r.jsx)(e.li,{children:"under whose authority"}),"\n",(0,r.jsx)(e.li,{children:"with which execution context"}),"\n",(0,r.jsx)(e.li,{children:"whether human approval was required"}),"\n",(0,r.jsx)(e.li,{children:"whether authority can be independently verified"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:["Gantral does not evaluate model quality, reasoning correctness, or business logic.\nIt governs ",(0,r.jsx)(e.strong,{children:"whether execution is allowed to proceed"}),"."]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"the-problem-gantral-solves",children:"The Problem Gantral Solves"}),"\n",(0,r.jsxs)(e.p,{children:["As AI systems move from experimentation into operational workflows,\norganizations lose control over ",(0,r.jsx)(e.strong,{children:"execution authority"}),", not just model behavior."]}),"\n",(0,r.jsx)(e.p,{children:"Human approval and accountability often exist only as convention:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"pull-request reviews"}),"\n",(0,r.jsx)(e.li,{children:"chat approvals"}),"\n",(0,r.jsx)(e.li,{children:"informal runbooks"}),"\n",(0,r.jsx)(e.li,{children:"screenshots and logs"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"These mechanisms:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"are not technically enforceable"}),"\n",(0,r.jsx)(e.li,{children:"are not bound to execution"}),"\n",(0,r.jsx)(e.li,{children:"do not produce durable, replayable evidence"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"At scale, this leads to:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"fragmented execution control across teams"}),"\n",(0,r.jsx)(e.li,{children:"approvals detached from execution context"}),"\n",(0,r.jsx)(e.li,{children:"post-incident reconstruction based on inference"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Gantral exists to make execution authority:"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"explicit \xb7 enforced \xb7 verifiable"})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"the-execution-authority-gap-before-and-after",children:"The Execution Authority Gap (Before and After)"}),"\n",(0,r.jsx)(e.p,{children:"Most AI governance failures are not caused by models.\nThey are caused by how execution authority is implicitly assumed."}),"\n",(0,r.jsx)(e.p,{children:"The diagrams below contrast the common \u201cbefore\u201d state\nwith an execution-time authority model."}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Before:"}),"\nIn this model, authority is implied by process and reconstructed after the fact."]}),"\n",(0,r.jsx)(e.mermaid,{value:"flowchart TB\n   A[AI Agent or Model\n   \u2022 Policy logic embedded in prompts or code\n   \u2022 Proposes or initiates actions autonomously]\n\n   A --\x3e M[Execution Management\n   \u2022 Automation workflows\n   \u2022 Task runners or orchestration\n   \u2022 Executes without explicit authority checks]\n\n   A -.-> H[Human Reviewer\n   \u2022 Informal notification or review\n   \u2022 No explicit authority boundary]\n\n   H --\x3e T[Human Action Tools\n   \u2022 Dashboards\n   \u2022 Ticketing systems\n   \u2022 Admin consoles]\n\n   M --\x3e R[Action in Real World\n   \u2022 Data changes\n   \u2022 Customer impact\n   \u2022 External side effects]\n\n   T --\x3e R\n\n   R -.-> L[Post-hoc Evidence\n   \u2022 Logs\n   \u2022 Tickets\n   \u2022 Chat records\n   \u2022 Human memory]"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"After:"})}),"\n",(0,r.jsx)(e.mermaid,{value:'flowchart TB\n   %% Governance (Advisory Only)\n   P[Policy & Governance\n   \u2022 Intent & constraints\n   \u2022 Risk thresholds\n   \u2022 Escalation rules\n   \u2022 Advisory only]\n\n\n   %% AI & Agent Systems\n   subgraph AI["AI & Agent Systems"]\n       LG[LangGraph\n       \u2022 Stateful workflows\n       \u2022 Long-running graphs]\n\n\n       VE[Vellum\n       \u2022 Prompt systems\n       \u2022 LLM pipelines]\n\n\n       CC[Claude Cowork\n       \u2022 Digital coworker\n       \u2022 Tool-using agent]\n   end\n\n\n   %% Human Authority\n   H[Human Operator\n   \u2022 Reviews context\n   \u2022 Exercises judgment\n   \u2022 Accountable actor]\n\n\n   %% Execution Management\n   M[Execution Management\n   \u2022 Workflow orchestration\n   \u2022 Permission requests\n   \u2022 Awaits authority]\n\n\n   %% Gantral Authority Boundary\n   subgraph Gantral["Gantral \u2014 Execution-Time Authority"]\n       G[Commitment Boundary\n       \u2022 Pause execution\n       \u2022 Require explicit decision\n       \u2022 Approve / Reject / Escalate\n       \u2022 Record authority deterministically]\n   end\n\n\n   %% Authorized Executors\n   subgraph X["Authorized Executors"]\n       XR[Automated Executors\n       \u2022 Jobs\n       \u2022 Pipelines\n       \u2022 Agent runners]\n\n\n       HX[Human Executors\n       \u2022 CLI\n       \u2022 UI\n       \u2022 Manual actions]\n   end\n\n\n   %% Real World & Evidence\n   R[Real-World Actions\n   \u2022 Code changes\n   \u2022 File edits\n   \u2022 Infra updates\n   \u2022 External effects]\n\n\n   E[Execution Evidence\n   \u2022 Logs - automated\n   \u2022 Commits / Artifacts\n   \u2022 Tickets\n   \u2022 Human attestations]\n\n\n   D[Deterministic Authority Record\n   \u2022 Who authorized\n   \u2022 What was approved\n   \u2022 When & under what conditions\n   \u2022 References to execution evidence]\n\n\n   %% Flows\n   LG --\x3e M\n   VE --\x3e M\n   CC --\x3e|tool calls| M\n\n\n   H --\x3e M\n   P -.-> G\n\n\n   M --\x3e G\n\n\n   G --\x3e|Authorized| XR\n   G --\x3e|Authorized| HX\n\n\n   XR --\x3e R\n   HX --\x3e R\n\n\n   R --\x3e E\n\n\n   E -.-> D\n   G -.-> D\n\n\n   G --\x3e|Pause / Escalate| H'}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"the-core-distinction-authority-vs-intelligence",children:"The Core Distinction: Authority vs Intelligence"}),"\n",(0,r.jsxs)(e.p,{children:["Gantral introduces a strict separation between ",(0,r.jsx)(e.strong,{children:"Intelligence"})," and ",(0,r.jsx)(e.strong,{children:"Authority"}),"."]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Agents and AI systems"})," provide intelligence",(0,r.jsx)(e.br,{}),"\n","(planning, reasoning, proposing actions)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Gantral"})," enforces authority",(0,r.jsx)(e.br,{}),"\n","(whether execution may proceed, pause, resume, or terminate)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"This separation is structural, not conceptual."}),"\n",(0,r.jsx)(e.p,{children:"Agents may recommend actions.\nPolicies may advise escalation.\nOnly Gantral enforces execution authority as state."}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"how-execution-is-governed",children:"How Execution Is Governed"}),"\n",(0,r.jsxs)(e.p,{children:["Gantral does not host agents or workflows.\nIt operates as an ",(0,r.jsx)(e.strong,{children:"execution-time authority layer"}),"."]}),"\n",(0,r.jsx)(e.p,{children:"A typical flow:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"An agent proposes an action"}),"\n",(0,r.jsx)(e.li,{children:"Execution reaches a governed boundary"}),"\n",(0,r.jsxs)(e.li,{children:["Gantral evaluates policy as a ",(0,r.jsx)(e.em,{children:"guard"})]}),"\n",(0,r.jsxs)(e.li,{children:["Execution either:","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"continues"}),"\n",(0,r.jsx)(e.li,{children:"pauses for human authority"}),"\n",(0,r.jsx)(e.li,{children:"terminates"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.li,{children:"Authority decisions are enforced via a deterministic state machine"}),"\n",(0,r.jsx)(e.li,{children:"A commitment artifact is emitted when authority is exercised"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"If authority cannot be enforced and recorded,\nexecution must not proceed."}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"federated-execution-runner-model",children:"Federated Execution (Runner Model)"}),"\n",(0,r.jsxs)(e.p,{children:["Gantral uses a ",(0,r.jsx)(e.strong,{children:"federated runner pattern"}),"."]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Agents execute in team-owned infrastructure"}),"\n",(0,r.jsx)(e.li,{children:"Gantral does not inspect agent memory or tool payloads"}),"\n",(0,r.jsx)(e.li,{children:"Execution is paused or resumed via explicit authority transitions"}),"\n",(0,r.jsx)(e.li,{children:"Long waits allow agent processes to exit (zero CPU)"}),"\n",(0,r.jsx)(e.li,{children:"Resumption launches a new process with fresh context"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:["Gantral governs ",(0,r.jsx)(e.strong,{children:"execution permission"}),", not execution mechanics."]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"the-governance-failure-gantral-addresses",children:"The Governance Failure Gantral Addresses"}),"\n",(0,r.jsx)(e.p,{children:"Large organizations encounter two structural failures when scaling AI."}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h3,{id:"1-fragmented-execution-control",children:"1. Fragmented Execution Control"}),"\n",(0,r.jsx)(e.p,{children:"Without a shared execution authority layer:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"approval logic is embedded in prompts and scripts"}),"\n",(0,r.jsx)(e.li,{children:"safety checks are duplicated across teams"}),"\n",(0,r.jsx)(e.li,{children:"escalation paths are inconsistent"}),"\n",(0,r.jsx)(e.li,{children:"platform teams cannot reason about enforcement guarantees"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Gantral externalizes execution authority from agent logic,\nmaking it deterministic, centrally governed, and auditable."}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h3,{id:"2-broken-chain-of-authority",children:"2. Broken Chain of Authority"}),"\n",(0,r.jsx)(e.p,{children:"Even when humans are involved, authority is often disconnected from execution."}),"\n",(0,r.jsx)(e.p,{children:"Common failures:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"approvals based on summaries rather than execution context"}),"\n",(0,r.jsx)(e.li,{children:"manual execution outside the approval system"}),"\n",(0,r.jsxs)(e.li,{children:["logs that explain ",(0,r.jsx)(e.em,{children:"what happened"})," but not ",(0,r.jsx)(e.em,{children:"who authorized it"})]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Gantral binds:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"execution context references"}),"\n",(0,r.jsx)(e.li,{children:"authority decisions"}),"\n",(0,r.jsx)(e.li,{children:"execution state transitions"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"into a single, immutable execution record."}),"\n",(0,r.jsxs)(e.p,{children:["Gantral does not interpret evidence.\nIt enforces that ",(0,r.jsx)(e.strong,{children:"no governed action proceeds without recorded authority"}),"."]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"what-gantral-owns",children:"What Gantral Owns"}),"\n",(0,r.jsxs)(e.p,{children:["Gantral owns ",(0,r.jsx)(e.strong,{children:"execution authority"}),", not intelligence."]}),"\n",(0,r.jsx)(e.p,{children:"It provides:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"a deterministic authority state machine"}),"\n",(0,r.jsx)(e.li,{children:"blocking human-in-the-loop enforcement"}),"\n",(0,r.jsx)(e.li,{children:"instance-level execution isolation"}),"\n",(0,r.jsx)(e.li,{children:"policy-as-guard integration (e.g. OPA)"}),"\n",(0,r.jsx)(e.li,{children:"immutable, replayable authority records"}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"what-gantral-does-not-do",children:"What Gantral Does Not Do"}),"\n",(0,r.jsxs)(e.p,{children:["Gantral explicitly does ",(0,r.jsx)(e.strong,{children:"not"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"reason or plan"}),"\n",(0,r.jsx)(e.li,{children:"store or inspect agent memory"}),"\n",(0,r.jsx)(e.li,{children:"inspect tool payloads"}),"\n",(0,r.jsx)(e.li,{children:"author business logic"}),"\n",(0,r.jsx)(e.li,{children:"make autonomous decisions"}),"\n",(0,r.jsx)(e.li,{children:"guarantee correctness or compliance"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:["Gantral records ",(0,r.jsx)(e.strong,{children:"authority"}),", not intent, interpretation, or correctness."]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"how-to-think-about-gantral",children:"How to Think About Gantral"}),"\n",(0,r.jsx)(e.p,{children:"Useful mental models:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"\u201csudo for AI\u201d"}),(0,r.jsx)(e.br,{}),"\n","Execution is intercepted and requires authority before proceeding."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"\u201cControl plane for decisions\u201d"}),(0,r.jsx)(e.br,{}),"\n","Authority is enforced as state, not inferred from logs."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"\u201cChain of custody for execution\u201d"}),(0,r.jsx)(e.br,{}),"\n","Authority is bound to execution at the moment it occurs."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"when-gantral-is-appropriate",children:"When Gantral Is Appropriate"}),"\n",(0,r.jsx)(e.p,{children:"Gantral is designed for actions that:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"affect production systems"}),"\n",(0,r.jsx)(e.li,{children:"have regulatory, financial, or security impact"}),"\n",(0,r.jsx)(e.li,{children:"require explicit human accountability"}),"\n",(0,r.jsx)(e.li,{children:"must be audited months or years later"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Gantral is not necessary for:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"advisory-only agents"}),"\n",(0,r.jsx)(e.li,{children:"exploratory or sandbox workflows"}),"\n",(0,r.jsx)(e.li,{children:"low-impact or fully reversible actions"}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"where-to-go-next",children:"Where to Go Next"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Authority & Enforcement"}),(0,r.jsx)(e.br,{}),"\n","See the ",(0,r.jsx)(e.strong,{children:(0,r.jsx)(e.a,{href:"/architecture/authority-state-machine",children:"Authority State Machine"})})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Proof & Audit"}),(0,r.jsx)(e.br,{}),"\n","Start with ",(0,r.jsx)(e.strong,{children:(0,r.jsx)(e.a,{href:"/verifiability/",children:"Verifiability Overview"})})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Technical Semantics"}),(0,r.jsx)(e.br,{}),"\n","Read the ",(0,r.jsx)(e.strong,{children:(0,r.jsx)(e.a,{href:"/architecture/trd",children:"Technical Reference (TRD)"})})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.p,{children:"Gantral is intentionally narrow."}),"\n",(0,r.jsxs)(e.p,{children:["It does not try to make AI safe, correct, or ethical.\nIt makes ",(0,r.jsx)(e.strong,{children:"execution authority explicit, enforceable, and provable"}),"."]})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(h,{...n})}):h(n)}},8453(n,e,t){t.d(e,{R:()=>o,x:()=>a});var i=t(6540);const r={},s=i.createContext(r);function o(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);